{
 "metadata": {
  "name": "",
  "signature": "sha256:4cc40021360e69251f68cde29f4997cff4648a4ec6e5470adf61e9485fd0e754"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h1>Criteo CTR prediction</h1>\n",
      "\n",
      "By: Ryan Houck\n",
      "\n",
      "<h3>Summary:</h3>\n",
      "\n",
      "By training a stochastic gradient descent with a logistic regression loss function on the first one million rows of the Criteo training set, I was able to get an average log loss / cost of 0.48 with average accuracy of nearly 80% on the subsequent one hundred thousand rows (i.e. the test set).\n",
      "\n",
      "Although the data set used here was a subset of the full training set, the methodology for training the model is scalable to the full Criteo training set as the model was trained out of core, row by row on a hashed feature space.\n",
      "\n",
      "<h3>Explanation:</h3>\n",
      "\n",
      "The standard approach to machine learning problems is to load the data set in question into memory then iteratively train and test a model over randomly selected rows of the data set. Doing this enables a person to get a working model in short period of time that generalizes well by not over-fitting to certain sections of the data set.\n",
      "\n",
      "The Criteo training set, however, is tens of millions of rows long. Analyzing a data set of this size in memory isn't practical. Moreover, a company like Criteo is constantly generating new rows of data to analyze and whatever model is employed needs to update regularly to account for changing trends in the data.\n",
      "\n",
      "With this type of problem, it makes sense to try an online learning approach, also called \"out of core\" learning. Here, a model is trained and updated with every new data point as it comes in. Programs, such as Vowpal Wabbit, have been built to handle these sorts of environments. With Vowpal Wabbit, new rows of data are passed to the model as they come in (often user-generated data) and the model is immediately updated, one row at a time.\n",
      "\n",
      "Here I'll simulate online learning on a relatively small subset of the Criteo training set. Instead of using Vowpal Wabbit, I'll iterate over a CSV file and update the model line by line. The data set is kept short for demonstration purposes in this iPython Notebook but could easily be extended to the full data set.\n",
      "\n",
      "Because we'll be training the model line by line to simulate an online learning environment, we'll use a stochastic gradient descent algorithm to update model-fitting parameters. A challenge with this problem is that the majority of the data set's features are categorical (e.g. red, blue, green, rather than vectors: 1.0, 1.5, 2.5). With categorical features it's necessary to encode the features as a series of binary values.\n",
      "\n",
      "To continue with the color example, instead of storing the feature : value pair as:\n",
      "\n",
      "['color': 'red'],\n",
      "\n",
      "We'd need to encode the feature as:\n",
      "\n",
      "['red': 1, 'blue': 0, 'green': 0]\n",
      "\n",
      "The challenge with implementing this when you aren't able to store the entire data set into memory is it's impossible to know how many values of a given feature will be present. There could be ten colors or one thousand. People have handled these sorts of situations by implementing what is called the \"hashing trick\". With this, we assume from the start that each row of data has some very large number of features (e.g. one million). We won't know what these features represent ahead of time. Instead, each feature-value combination is mapped to it's own position in this feature space of some large number, one million in this example. The hope then is that there are fewer than one million unique feature : value combinations. If there are more than one million unique combinations, some feature-values will be lumped together and treated as the same by the machine-learning algorithm. The hope then is that this occurs infrequently enough that it has little overall impact on model performance. Although the first 13 features in the training set are integer values, not categorical, it is possible to treat them as categories and apply the same hashing trick to the entire training set.\n",
      "\n",
      "I've chosen to use a Logistic Regression cost function to train and update the model. Logistic regression is one of the most widely used classification algorithms in part because it handles large feature spaces well. It works by applying a sigmoid function to a given linear regression hypothesis (H = m0 + m1X1 + m2X2 + m3X3) so that all predictions represent some probability that the true value is 1. The model then heavily penalizes predictions that are far off.\n",
      "\n",
      "For example:\n",
      "\n",
      "If the model predicts with 95% confidence that the true value is 1, and it turns out to be correct, the penalty is small. If the model predicts with 95% confidence that the true value is 1, and it turns out to be incorrect, the penalty is very large. If it predicts with 50% confidence that the true value is 1, the penalty will be somewhere in the middle.\n",
      "\n",
      "Next I'll explain how the script below operates.\n",
      "\n",
      "<h3>Methodology:</h3>\n",
      "    1. Instantiate stochastic gradient descent classifier algorithm with logistic regression loss function\n",
      "    2. For each row in data set:\n",
      "        1. Remove target feature from row and set as 'y' (y is 1 or 0, representing clicked or not clicked, respectively)\n",
      "        2. Hash all remaining feature : value pairs into sparse matrix of sufficiently high dimensional space\n",
      "        3. Apply logistic regression algorithm to predict probability that row represents a clicked-ad (i.e. y = 1)\n",
      "        4. Compare prediction to actual target value, y, and record both the cost function (log loss) and accuracy\n",
      "        5. Update/train model on row, adjusting parameters to reflect the model prediction performance on the row\n",
      "        6. Drop the row"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# import dependencies\n",
      "import csv\n",
      "from datetime import datetime\n",
      "from math import log\n",
      " \n",
      "%matplotlib inline\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      " \n",
      "from sklearn.feature_extraction import FeatureHasher\n",
      "from sklearn.linear_model import SGDClassifier"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 75
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For purposes of demonstration, I'm working with a truncated criteo data set. It can be assumed that the model will only improve in performance with the full data set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# truncate criteo train data set to first 1,000,000 lines to iterate over\n",
      "train = 'source/train_small.txt'\n",
      "reader = csv.reader( open( train, 'r' ), delimiter = '\\t' )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 76
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These functions are used to calibrate the model and format data for SGD / logistic regression algorithm."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# this is the cost function for logistic regression\n",
      "# this is calculated for every iteration over the data set\n",
      "# the machine learning algorithm adjusts it's parameters in an attempt to reduce this value over time to a global minimum\n",
      "def logloss(p, y):\n",
      "    p = max(min(p, 1. - 10e-12), 10e-12)\n",
      "    return -log(p) if y == 1. else -log(1. - p)\n",
      " \n",
      "# this is used to test the accuracy of the logistic regression algorithm\n",
      "# the algorithm estimates the probability that a feature vector (row) represents a clicked advertisement\n",
      "# if this probability is greater than 0.5, the model estimates it is a clicked advertisement\n",
      "def prediction(p):\n",
      "    pred = 1. if p >= 0.5 else 0.\n",
      "    return pred \n",
      " \n",
      "# this function returns a sparce matrix containing hashed feature:value pairs for each feature vector (row)\n",
      "# with online (iterative) learning, it's impossible to know the full range of categorical values for a feature ahead of time\n",
      "# to get around this, we create new features for each unique feature:value pairs by storing hashed values to represent indices in a large vector\n",
      "def hash_x(csv_row, hasher):\n",
      "    x = []\n",
      "    for count, value in enumerate(csv_row):\n",
      "        if value:\n",
      "            x.append('F%s:%s' % (count,value))\n",
      "    \n",
      "    x = hasher.transform([x])\n",
      "    return x"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 77
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Set parameters and performance-tracking variables for main loop "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# this will keep track of the average cost function over time \n",
      "# the aim is to minimize this\n",
      "loss = 0.\n",
      " \n",
      "# this will keep track of average prediction accuracy over time\n",
      "# the aim is to maximize this\n",
      "accuracy = 0.\n",
      " \n",
      "# this is the logistic regression classifier model\n",
      "clf = SGDClassifier(loss='log', alpha=0.001)\n",
      " \n",
      "# this is the hashing algorithm we'll be using to map the feature set of unknown complexity\n",
      "hasher = FeatureHasher(input_type='string', n_features=(2 ** 15))\n",
      " \n",
      "# this matrix will store periodic summarizing data to track algorithm performance over time\n",
      "chart_data = np.empty((0,3), float)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 78
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Train model on data set by predicting probability each row represents a clicked-ad, then compairing prediction to actual value and adjusting the model when predicted probabilty does not match actual value."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# this is the main loop that iterates through the data set and trains the model one row at a time\n",
      "for t, row in enumerate(reader):\n",
      "      \n",
      "    y = 1. if row[0] == '1' else 0.\n",
      "    # delete 'answer' so model doesn't train with it visible\n",
      "    del row[0]\n",
      "    \n",
      "    # step 1, get the hashed features\n",
      "    x = hash_x(row, hasher)\n",
      "    \n",
      "    if t > 1:\n",
      "        \n",
      "        # estimate liklihood classification is 1\n",
      "        prob = clf.predict_proba(x)[0][1]\n",
      "        # estimate classification\n",
      "        pred = prediction(prob)\n",
      "        # calc cost function and increment\n",
      "        loss += logloss(prob, y)\n",
      "        # add one point for correct prediction\n",
      "        accuracy += 1. if pred == y else 0.\n",
      "            \n",
      "        # checkpoint at every 100,000 rows to confirm model is improving and at an acceptable pace\n",
      "        if t % 100000 == 0:\n",
      "            avg_loss = loss/t\n",
      "            avg_accuracy = accuracy/t\n",
      "            print('%s\\trows seen: %d\\tcurrent logloss: %f\\tavg accuracy: %f' % (datetime.now(), t, avg_loss, avg_accuracy))\n",
      "            # save checkpoint stats for plotting performance after loop completion\n",
      "            chart_data = np.append(chart_data, np.array([[t/1000,avg_loss,avg_accuracy]]), axis=0)\n",
      " \n",
      "    # fit model for one row\n",
      "    clf.partial_fit(x, np.asarray([y]), np.asarray([0.,1.]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2014-10-20 20:21:52.033054\trows seen: 100000\tcurrent logloss: 0.533604\tavg accuracy: 0.783120\n",
        "2014-10-20 20:22:42.989606\trows seen: 200000\tcurrent logloss: 0.507637\tavg accuracy: 0.778890"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-10-20 20:23:32.705374\trows seen: 300000\tcurrent logloss: 0.504537\tavg accuracy: 0.773697"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-10-20 20:24:22.505036\trows seen: 400000\tcurrent logloss: 0.502264\tavg accuracy: 0.771948"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-10-20 20:25:11.490229\trows seen: 500000\tcurrent logloss: 0.500818\tavg accuracy: 0.770924"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-10-20 20:25:58.245442\trows seen: 600000\tcurrent logloss: 0.499023\tavg accuracy: 0.770890"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-10-20 20:26:45.729643\trows seen: 700000\tcurrent logloss: 0.496973\tavg accuracy: 0.771170"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-10-20 20:27:33.342143\trows seen: 800000\tcurrent logloss: 0.495100\tavg accuracy: 0.772044"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-10-20 20:28:20.425736\trows seen: 900000\tcurrent logloss: 0.493576\tavg accuracy: 0.772453"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2014-10-20 20:29:08.049656\trows seen: 1000000\tcurrent logloss: 0.492135\tavg accuracy: 0.773023"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 79
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For each row in the data set, the model calculates a penalty or cost for its prediction error. Because the model can never predict the true value with 100% certainty, there will always be some error for each prediction, however small. The model updates its weights / parameters at every iteration in attempt to reduce this cost for the next prediction.\n",
      "\n",
      "If the model is learning, its average cost should decrease over time. This average cost is plotted in the first chart. Similarly, the ratio of correct predictions should increase over time. The ratio of correct predictions is called \"accuracy\" is plotted in the second chart."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# print chart   \n",
      "chart_data_frame = pd.DataFrame({\n",
      "                                'Avg Loss': chart_data[:,1],\n",
      "                                'Avg Accuracy': chart_data[:,2]\n",
      "                                }, \n",
      "                                index=chart_data[:,0])  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 80
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# chart avg cost function value over time\n",
      "log_chart = chart_data_frame.ix[:,'Avg Loss'].plot()\n",
      "log_chart.set_xlabel('Iterations 000s')\n",
      "log_chart.set_ylabel('Avg Log Loss')\n",
      "log_chart.set_title('Plot of Avg Cost Function Over Time')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 82,
       "text": [
        "<matplotlib.text.Text at 0x10dea2d90>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEZCAYAAABWwhjiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x10c64e590>"
       ]
      }
     ],
     "prompt_number": 82
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Test fitted model on test set"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# test on subsequent 100k rows of criteo training data set\n",
      "train = 'source/test_small.txt'\n",
      "reader = csv.reader( open( train, 'r' ), delimiter = '\\t' )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 84
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# reset performance trackers\n",
      "accuracy = 0\n",
      "loss = 0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 85
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# this loop iterates through the test set and records model performance\n",
      "for t, row in enumerate(reader):\n",
      "      \n",
      "    y = 1. if row[0] == '1' else 0.\n",
      "    # delete 'answer'\n",
      "    del row[0]\n",
      "    \n",
      "    # step 1, get the hashed features\n",
      "    x = hash_x(row, hasher)\n",
      "        \n",
      "    # estimate liklihood classification is 1\n",
      "    prob = clf.predict_proba(x)[0][1]\n",
      "    # estimate classification\n",
      "    pred = prediction(prob)\n",
      "    # calc cost function and increment\n",
      "    loss += logloss(prob, y)\n",
      "    # add one point for correct prediction\n",
      "    accuracy += 1. if pred == y else 0."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 86
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Avg Log Loss: %0.2f\" % (loss / t)\n",
      "print \"Avg Accuracy: %0.2f\" % (accuracy / t)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Avg Log Loss: 0.48\n",
        "Avg Accuracy: 0.78\n"
       ]
      }
     ],
     "prompt_number": 88
    }
   ],
   "metadata": {}
  }
 ]
}